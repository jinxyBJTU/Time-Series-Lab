# [Publications in ICLR 2024](https://openreview.net/group?id=ICLR.cc/2024/Conference#tab-accept-oral)

## Time Series Forecasting
### iTransformer: Inverted Transformers Are Effective for Time Series Forecasting 
 - [paper](https://openreview.net/attachment?id=JePfAI8fah&name=pdf)
 - 最近线性预测模型的繁荣对基于 Transformer 的预测器的架构修改的持续热情提出了质疑。 这些预测器利用 Transformer 对时间序列的时间标记 *temporal tokens* 的全局依赖关系进行建模，每个标记由同一时间戳的多个变量形成。 然而，由于性能下降和计算爆炸，Transformers 在预测具有较大**回溯窗口**的序列时面临挑战。 此外，每个时间标记的统一嵌入*embedding*融合了具有潜在未对齐时间戳和不同物理测量的多个变量，这可能无法学习以变量为中心的表示并导致无意义的注意力图。 在这项工作中，我们反思了 Transformer 组件的职责，并在不对基本组件进行任何修改的情况下重新调整了 Transformer 架构的用途。 我们提出 **iTransformer**，它简单地将注意力和前馈网络应用于反转维度。 具体来说，各个系列的时间点被嵌入到变量标记中，注意力机制利用变量标记来捕获多元相关性； 同时，前馈网络应用于每个变量标记来学习非线性表示。 iTransformer 模型在具有挑战性的现实世界数据集上达到了最先进的水平，这进一步增强了 Transformer 系列的性能、跨不同变量的泛化能力，以及更好地利用任意回溯窗口，使其成为一个很好的替代方案 时间序列预测的基本支柱。

## Representation Learning
### Soft Contrastive Learning for Time Series
- [paper](https://openreview.net/attachment?id=pAsQSWlDUf&name=pdf)
- 对比学习已被证明可以有效地以自我监督的方式学习时间序列的表示。 然而，对比时间序列中相邻时间戳的相似时间序列实例或值会导致忽略它们固有的相关性，从而导致学习表示的质量恶化。 为了解决这个问题，我们提出了 SoftCLT，一种简单而有效的时间序列软对比学习策略。 这是通过引入实例和时间对比损失以及范围从 0 到 1 的软分配来实现的。 具体来说，我们定义了软分配：1）按数据空间上时间序列之间的距离计算的实例对比损失，以及2）按时间戳差异计算的时间对比损失。 SoftCLT 是一种用于时间序列对比学习的即插即用方法，可以提高学习表示的质量，而无需附加任何附加功能。 在实验中，我们证明 SoftCLT 持续提高了各种下游任务的性能，包括分类、半监督学习、迁移学习和异常检测，显示出最先进的性能。
